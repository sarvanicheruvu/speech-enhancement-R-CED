{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "implementation",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "SArsBcgX-Phk",
        "outputId": "a1be1c91-1d0d-418e-a09f-03715003495f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4l2hdBvX969i",
        "outputId": "6e53a9d1-6e79-4c6b-dbfe-3e2de398b753",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 939
        }
      },
      "source": [
        "# !pip uninstall tensorflow\n",
        "#!pip3 install --upgrade tensorflow-gpu\n",
        "!pip install --upgrade tensorflow\n",
        "# !pip install grpcio>=1.24.3 --upgrade\n",
        "# # !pip install -U keras-tuner\n",
        "# !pip install tensorflow-addons\n",
        "!pip install keras-rectified-adam"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: tensorflow in /usr/local/lib/python3.6/dist-packages (2.3.0)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.9.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied, skipping upgrade: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.34.2)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.31.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tensorflow) (49.2.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.17.2)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.1.1)\n",
            "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.6)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.4.8)\n",
            "Collecting keras-rectified-adam\n",
            "  Downloading https://files.pythonhosted.org/packages/21/79/9521f66b92186702cb58a214c1b923b416266381cd824e15a1733f6a5b06/keras-rectified-adam-0.17.0.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-rectified-adam) (1.18.5)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras-rectified-adam) (2.4.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras-rectified-adam) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras-rectified-adam) (2.10.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-rectified-adam) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->Keras->keras-rectified-adam) (1.15.0)\n",
            "Building wheels for collected packages: keras-rectified-adam\n",
            "  Building wheel for keras-rectified-adam (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-rectified-adam: filename=keras_rectified_adam-0.17.0-cp36-none-any.whl size=14781 sha256=f2ad7ceaab6cf32d4760b17357f421048b88d6cc910f4a713ac06c59b2f6227d\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/01/27/3a934e1a5644f5b93c720422a6ef97034ea78a21ba71cfb549\n",
            "Successfully built keras-rectified-adam\n",
            "Installing collected packages: keras-rectified-adam\n",
            "Successfully installed keras-rectified-adam-0.17.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feMruIwg-F15"
      },
      "source": [
        "from keras_radam import RAdam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlHGYY9L-Idt"
      },
      "source": [
        "import librosa\n",
        "import pandas as pd\n",
        "import os\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import IPython.display as ipd\n",
        "import librosa.display\n",
        "import scipy\n",
        "import glob\n",
        "import numpy as np\n",
        "import math\n",
        "import warnings\n",
        "import pickle\n",
        "from sklearn.utils import shuffle\n",
        "from keras.backend import clear_session\n",
        "# Before instantiating a tf.data.Dataset obj & before model creation, call:\n",
        "clear_session()\n",
        "\n",
        "# Load the TensorBoard notebook extension.\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0BsLOCg-Lzk"
      },
      "source": [
        "tf.random.set_seed(999)\n",
        "np.random.seed(999)\n",
        "\n",
        "#setting global seeds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHCz0FaC-RpL",
        "outputId": "98675dff-3b82-436e-d52e-d1dc8f27cbbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 624
        }
      },
      "source": [
        "!ls '/content/drive/My Drive/daitan/implementation/largescale/records'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "val_0.tfrecords    val_132.tfrecords  val_38.tfrecords\tval_6.tfrecords\n",
            "val_100.tfrecords  val_133.tfrecords  val_39.tfrecords\tval_70.tfrecords\n",
            "val_101.tfrecords  val_134.tfrecords  val_3.tfrecords\tval_71.tfrecords\n",
            "val_102.tfrecords  val_135.tfrecords  val_40.tfrecords\tval_72.tfrecords\n",
            "val_103.tfrecords  val_136.tfrecords  val_41.tfrecords\tval_73.tfrecords\n",
            "val_104.tfrecords  val_137.tfrecords  val_42.tfrecords\tval_74.tfrecords\n",
            "val_105.tfrecords  val_138.tfrecords  val_43.tfrecords\tval_75.tfrecords\n",
            "val_106.tfrecords  val_139.tfrecords  val_44.tfrecords\tval_76.tfrecords\n",
            "val_107.tfrecords  val_13.tfrecords   val_45.tfrecords\tval_77.tfrecords\n",
            "val_108.tfrecords  val_14.tfrecords   val_46.tfrecords\tval_78.tfrecords\n",
            "val_109.tfrecords  val_15.tfrecords   val_47.tfrecords\tval_79.tfrecords\n",
            "val_10.tfrecords   val_16.tfrecords   val_48.tfrecords\tval_7.tfrecords\n",
            "val_110.tfrecords  val_17.tfrecords   val_49.tfrecords\tval_80.tfrecords\n",
            "val_111.tfrecords  val_18.tfrecords   val_4.tfrecords\tval_81.tfrecords\n",
            "val_112.tfrecords  val_19.tfrecords   val_50.tfrecords\tval_82.tfrecords\n",
            "val_113.tfrecords  val_1.tfrecords    val_51.tfrecords\tval_83.tfrecords\n",
            "val_114.tfrecords  val_20.tfrecords   val_52.tfrecords\tval_84.tfrecords\n",
            "val_115.tfrecords  val_21.tfrecords   val_53.tfrecords\tval_85.tfrecords\n",
            "val_116.tfrecords  val_22.tfrecords   val_54.tfrecords\tval_86.tfrecords\n",
            "val_117.tfrecords  val_23.tfrecords   val_55.tfrecords\tval_87.tfrecords\n",
            "val_118.tfrecords  val_24.tfrecords   val_56.tfrecords\tval_88.tfrecords\n",
            "val_119.tfrecords  val_25.tfrecords   val_57.tfrecords\tval_89.tfrecords\n",
            "val_11.tfrecords   val_26.tfrecords   val_58.tfrecords\tval_8.tfrecords\n",
            "val_120.tfrecords  val_27.tfrecords   val_59.tfrecords\tval_90.tfrecords\n",
            "val_121.tfrecords  val_28.tfrecords   val_5.tfrecords\tval_91.tfrecords\n",
            "val_122.tfrecords  val_29.tfrecords   val_60.tfrecords\tval_92.tfrecords\n",
            "val_123.tfrecords  val_2.tfrecords    val_61.tfrecords\tval_93.tfrecords\n",
            "val_124.tfrecords  val_30.tfrecords   val_62.tfrecords\tval_94.tfrecords\n",
            "val_125.tfrecords  val_31.tfrecords   val_63.tfrecords\tval_95.tfrecords\n",
            "val_127.tfrecords  val_32.tfrecords   val_64.tfrecords\tval_96.tfrecords\n",
            "val_128.tfrecords  val_33.tfrecords   val_65.tfrecords\tval_97.tfrecords\n",
            "val_129.tfrecords  val_34.tfrecords   val_66.tfrecords\tval_98.tfrecords\n",
            "val_12.tfrecords   val_35.tfrecords   val_67.tfrecords\tval_99.tfrecords\n",
            "val_130.tfrecords  val_36.tfrecords   val_68.tfrecords\tval_9.tfrecords\n",
            "val_131.tfrecords  val_37.tfrecords   val_69.tfrecords\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZ4MtDj0-ab_"
      },
      "source": [
        "train_tfrecords_filenames = glob.glob('/content/drive/My Drive/daitan/implementation/largescale/records/train_*')\n",
        "np.random.shuffle(train_tfrecords_filenames)\n",
        "train_tfrecords_filenames = list(train_tfrecords_filenames)\n",
        "val_tfrecords_filenames = glob.glob('/content/drive/My Drive/daitan/implementation/largescale/records/val_*')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Tn1XfGd-cvQ",
        "outputId": "29b74aa3-cef4-4bef-f2cb-7cc26e475b89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "windowLength = 256\n",
        "overlap      = round(0.25 * windowLength) # overlap of 75%\n",
        "ffTLength    = windowLength\n",
        "inputFs      = 48e3\n",
        "fs           = 16e3\n",
        "numFeatures  = ffTLength//2 + 1\n",
        "numSegments  = 8\n",
        "print(\"windowLength:\",windowLength)\n",
        "print(\"overlap:\",overlap)\n",
        "print(\"ffTLength:\",ffTLength)\n",
        "print(\"inputFs:\",inputFs)\n",
        "print(\"fs:\",fs)\n",
        "print(\"numFeatures:\",numFeatures)\n",
        "print(\"numSegments:\",numSegments)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "windowLength: 256\n",
            "overlap: 64\n",
            "ffTLength: 256\n",
            "inputFs: 48000.0\n",
            "fs: 16000.0\n",
            "numFeatures: 129\n",
            "numSegments: 8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKRLw8PH-eMG"
      },
      "source": [
        "def tf_record_parser(record):\n",
        "    keys_to_features = {\n",
        "        \"noise_stft_phase\": tf.io.FixedLenFeature((), tf.string, default_value=\"\"),\n",
        "        'noise_stft_mag_features': tf.io.FixedLenFeature([], tf.string),\n",
        "        \"clean_stft_magnitude\": tf.io.FixedLenFeature((), tf.string)\n",
        "    }\n",
        "\n",
        "    features = tf.io.parse_single_example(record, keys_to_features)\n",
        "\n",
        "    noise_stft_mag_features = tf.io.decode_raw(features['noise_stft_mag_features'], tf.float32)\n",
        "    clean_stft_magnitude = tf.io.decode_raw(features['clean_stft_magnitude'], tf.float32)\n",
        "    noise_stft_phase = tf.io.decode_raw(features['noise_stft_phase'], tf.float32)\n",
        "\n",
        "    # reshape input and annotation images\n",
        "    noise_stft_mag_features = tf.reshape(noise_stft_mag_features, (129, 8,1), name=\"noise_stft_mag_features\")\n",
        "    clean_stft_magnitude = tf.reshape(clean_stft_magnitude, (129,1, 1), name=\"clean_stft_magnitude\")\n",
        "    noise_stft_phase = tf.reshape(noise_stft_phase, (129,), name=\"noise_stft_phase\")\n",
        "    print(noise_stft_mag_features,clean_stft_magnitude)\n",
        "    return noise_stft_mag_features,clean_stft_magnitude"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyU_YBqr-kUx"
      },
      "source": [
        "Create tf.data.Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5FnNNtD-gtI"
      },
      "source": [
        "train_dataset = tf.data.TFRecordDataset([train_tfrecords_filenames])\n",
        "train_dataset = train_dataset.map(tf_record_parser)\n",
        "#train_dataset = train_dataset.shuffle(8192)\n",
        "train_dataset = train_dataset.repeat()\n",
        "train_dataset = train_dataset.batch(100)\n",
        "train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "print(train_dataset.element_spec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShqY54-D-rhY"
      },
      "source": [
        "test_dataset = tf.data.TFRecordDataset([val_tfrecords_filenames])\n",
        "test_dataset = test_dataset.map(tf_record_parser)\n",
        "test_dataset = test_dataset.repeat()\n",
        "test_dataset = test_dataset.batch(100)\n",
        "test_dataset = test_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "test_dataset.element_spec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QzTtC6Q-uhX"
      },
      "source": [
        "Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6LIoBHu-tUK"
      },
      "source": [
        "from tensorflow.keras.layers import Conv2D, Input, LeakyReLU, Flatten, Dense, Reshape, Conv2DTranspose, BatchNormalization, Activation, Dropout\n",
        "from tensorflow.keras import Model, Sequential"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xH1OoUjo-0Hw"
      },
      "source": [
        "def conv_block(x, filters, kernel_size, strides, padding='same', use_bn=True):\n",
        "  x = Conv2D(filters=filters, kernel_size=kernel_size, strides=strides, padding=padding, use_bias=False,\n",
        "              kernel_regularizer=tf.keras.regularizers.l1_l2(0,0))(x)\n",
        "  x = Activation('relu')(x)\n",
        "  if use_bn:\n",
        "    x = BatchNormalization()(x)\n",
        "  return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hE3Kvtod-1lM"
      },
      "source": [
        "def full_pre_activation_block(x, filters, kernel_size, strides, padding='same', use_bn=True):\n",
        "  shortcut = x\n",
        "  in_channels = x.shape[-1]\n",
        "\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = Conv2D(filters=filters, kernel_size=kernel_size, strides=strides, padding='same')(x)\n",
        "\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = Conv2D(filters=in_channels, kernel_size=kernel_size, strides=strides, padding='same')(x)\n",
        "\n",
        "  return shortcut + x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoa42hY_-4YX"
      },
      "source": [
        "def build_model(l1_strength,l2_strength):\n",
        "  inputs = Input(shape=[numFeatures,numSegments,1])\n",
        "  x = inputs\n",
        "  #x=Reshape((129,8,1), input_shape=(129,8,1,1))(x)\n",
        "  # -----\n",
        "  x = tf.keras.layers.ZeroPadding2D(((4,4), (0,0)))(x)\n",
        "  x = Conv2D(filters=18, kernel_size=[9,8], strides=[1, 1], padding='valid', use_bias=False,\n",
        "              kernel_regularizer=tf.keras.regularizers.l1_l2(l1_strength, l2_strength))(x)\n",
        "  x = Activation('relu')(x)\n",
        "  #x = Dropout(0.5)(x)\n",
        "  x = BatchNormalization()(x)\n",
        " \n",
        "  skip0 = Conv2D(filters=30, kernel_size=[5,1], strides=[1, 1], padding='same', use_bias=False,\n",
        "                 kernel_regularizer=tf.keras.regularizers.l1_l2(l1_strength, l2_strength))(x)\n",
        "  x = Activation('relu')(skip0)\n",
        "  x = BatchNormalization()(x)\n",
        " \n",
        "  x = Conv2D(filters=8, kernel_size=[9,1], strides=[1, 1], padding='same', use_bias=False,\n",
        "              kernel_regularizer=tf.keras.regularizers.l1_l2(l1_strength, l2_strength))(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = BatchNormalization()(x)\n",
        " \n",
        "  # -----\n",
        "  x = Conv2D(filters=18, kernel_size=[9,1], strides=[1, 1], padding='same', use_bias=False,\n",
        "              kernel_regularizer=tf.keras.regularizers.l1_l2(l1_strength, l2_strength))(x)\n",
        "  x = Activation('relu')(x)\n",
        "  #x = Dropout(0.5)(x)\n",
        "  x = BatchNormalization()(x)\n",
        " \n",
        "  skip1 = Conv2D(filters=30, kernel_size=[5,1], strides=[1, 1], padding='same', use_bias=False,\n",
        "                 kernel_regularizer=tf.keras.regularizers.l1_l2(l1_strength, l2_strength))(x)\n",
        "  x = Activation('relu')(skip1)\n",
        "  x = BatchNormalization()(x)\n",
        " \n",
        "  x = Conv2D(filters=8, kernel_size=[9,1], strides=[1, 1], padding='same', use_bias=False,\n",
        "              kernel_regularizer=tf.keras.regularizers.l1_l2(l1_strength, l2_strength))(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = BatchNormalization()(x)\n",
        " \n",
        "  # ----\n",
        "  x = Conv2D(filters=18, kernel_size=[9,1], strides=[1, 1], padding='same', use_bias=False,\n",
        "              kernel_regularizer=tf.keras.regularizers.l1_l2(l1_strength, l2_strength))(x)\n",
        "  x = Activation('relu')(x)\n",
        "  #x = Dropout(0.5)(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  \n",
        "  x = Conv2D(filters=30, kernel_size=[5,1], strides=[1, 1], padding='same', use_bias=False,\n",
        "              kernel_regularizer=tf.keras.regularizers.l1_l2(l1_strength, l2_strength))(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = BatchNormalization()(x)\n",
        " \n",
        "  x = Conv2D(filters=8, kernel_size=[9,1], strides=[1, 1], padding='same', use_bias=False,\n",
        "              kernel_regularizer=tf.keras.regularizers.l1_l2(l1_strength, l2_strength))(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = BatchNormalization()(x)\n",
        " \n",
        "  # ----\n",
        "  x = Conv2D(filters=18, kernel_size=[9,1], strides=[1, 1], padding='same', use_bias=False,\n",
        "              kernel_regularizer=tf.keras.regularizers.l1_l2(l1_strength, l2_strength))(x)\n",
        "  x = Activation('relu')(x)\n",
        "  #x = Dropout(0.5)(x)\n",
        "  x = BatchNormalization()(x)\n",
        " \n",
        "  x = Conv2D(filters=30, kernel_size=[5,1], strides=[1, 1], padding='same', use_bias=False,\n",
        "             kernel_regularizer=tf.keras.regularizers.l1_l2(l1_strength, l2_strength))(x)\n",
        "  #x = x + skip1\n",
        "  x = Activation('relu')(x)\n",
        "  x = BatchNormalization()(x)\n",
        " \n",
        "  x = Conv2D(filters=8, kernel_size=[9,1], strides=[1, 1], padding='same', use_bias=False,\n",
        "              kernel_regularizer=tf.keras.regularizers.l1_l2(l1_strength, l2_strength))(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = BatchNormalization()(x)\n",
        " \n",
        "  # ----\n",
        "  x = Conv2D(filters=18, kernel_size=[9,1], strides=[1, 1], padding='same', use_bias=False,\n",
        "              kernel_regularizer=tf.keras.regularizers.l1_l2(l1_strength, l2_strength))(x)\n",
        "  x = Activation('relu')(x)\n",
        "  #x = Dropout(0.5)(x)\n",
        "  x = BatchNormalization()(x)\n",
        " \n",
        "  x = Conv2D(filters=30, kernel_size=[5,1], strides=[1, 1], padding='same', use_bias=False,\n",
        "             kernel_regularizer=tf.keras.regularizers.l1_l2(l1_strength, l2_strength))(x)\n",
        "  #x = x + skip0\n",
        "  x = Activation('relu')(x)\n",
        "  x = BatchNormalization()(x)\n",
        " \n",
        "  x = Conv2D(filters=8, kernel_size=[9,1], strides=[1, 1], padding='same', use_bias=False,\n",
        "              kernel_regularizer=tf.keras.regularizers.l1_l2(l1_strength, l2_strength))(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = BatchNormalization()(x)\n",
        " \n",
        "  # ----\n",
        "  x = tf.keras.layers.SpatialDropout2D(0.2)(x)\n",
        "  x = Conv2D(filters=1, kernel_size=[129,1], strides=[1, 1], padding='same')(x)\n",
        " \n",
        "  model = Model(inputs=inputs, outputs=x)\n",
        " \n",
        "  optimizer = tf.keras.optimizers.Adam(3e-4)\n",
        "  #optimizer = RAdam(total_steps=10000, warmup_proportion=0.1, min_lr=3e-4)\n",
        " \n",
        "  model.compile(optimizer=optimizer, loss='mse', \n",
        "                metrics=[tf.keras.metrics.RootMeanSquaredError('rmse')])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MTqfWn9-6ES"
      },
      "source": [
        "model = build_model(l1_strength=0,l2_strength=0)\n",
        "model.summary()\n",
        "model.save('/content/drive/My Drive/daitan/implementation/largescale/models/models4.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOUsQZUQ--k7"
      },
      "source": [
        "def l2_norm(vector):\n",
        "    return np.square(vector)\n",
        "\n",
        "def SDR(denoised, cleaned, eps=1e-7): # Signal to Distortion Ratio\n",
        "    a = l2_norm(denoised)\n",
        "    b = l2_norm(denoised - cleaned)\n",
        "    a_b = a / b\n",
        "    return np.mean(10 * np.log10(a_b + eps))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xRkY6wJ3pQ_"
      },
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "saved_model= load_model('/content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxkz3krP_CTR",
        "outputId": "18acfdab-dfc3-4a7e-eff4-bb104afc9bf0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, update_freq='batch')\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath='/content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5', \n",
        "                                                         monitor='rmse',verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
        "model.fit(test_dataset,\n",
        "         steps_per_epoch=150,\n",
        "         #validation_data=test_dataset,\n",
        "         epochs=25000,\n",
        "         callbacks=[tensorboard_callback,checkpoint_callback]\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/25000\n",
            "  2/150 [..............................] - ETA: 14s - loss: 2.1907 - rmse: 1.4801WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0245s vs `on_train_batch_end` time: 0.1727s). Check your callbacks.\n",
            "149/150 [============================>.] - ETA: 0s - loss: 0.7926 - rmse: 0.8903\n",
            "Epoch 00001: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 28ms/step - loss: 0.7904 - rmse: 0.8890\n",
            "Epoch 2/25000\n",
            "150/150 [==============================] - ETA: 0s - loss: 0.3417 - rmse: 0.5846\n",
            "Epoch 00002: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 5s 30ms/step - loss: 0.3417 - rmse: 0.5846\n",
            "Epoch 3/25000\n",
            "148/150 [============================>.] - ETA: 0s - loss: 0.2972 - rmse: 0.5451\n",
            "Epoch 00003: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 5s 31ms/step - loss: 0.2972 - rmse: 0.5451\n",
            "Epoch 4/25000\n",
            "149/150 [============================>.] - ETA: 0s - loss: 0.2796 - rmse: 0.5288\n",
            "Epoch 00004: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 0.2788 - rmse: 0.5280\n",
            "Epoch 5/25000\n",
            "149/150 [============================>.] - ETA: 0s - loss: 0.2768 - rmse: 0.5262\n",
            "Epoch 00005: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 24ms/step - loss: 0.2764 - rmse: 0.5258\n",
            "Epoch 6/25000\n",
            "149/150 [============================>.] - ETA: 0s - loss: 0.2333 - rmse: 0.4831\n",
            "Epoch 00006: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 28ms/step - loss: 0.2337 - rmse: 0.4834\n",
            "Epoch 7/25000\n",
            "148/150 [============================>.] - ETA: 0s - loss: 0.2227 - rmse: 0.4719\n",
            "Epoch 00007: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 28ms/step - loss: 0.2244 - rmse: 0.4737\n",
            "Epoch 8/25000\n",
            "150/150 [==============================] - ETA: 0s - loss: 0.2017 - rmse: 0.4492\n",
            "Epoch 00008: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 28ms/step - loss: 0.2017 - rmse: 0.4492\n",
            "Epoch 9/25000\n",
            "148/150 [============================>.] - ETA: 0s - loss: 0.1667 - rmse: 0.4083\n",
            "Epoch 00009: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 24ms/step - loss: 0.1665 - rmse: 0.4081\n",
            "Epoch 10/25000\n",
            "149/150 [============================>.] - ETA: 0s - loss: 0.1971 - rmse: 0.4439\n",
            "Epoch 00010: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 0.1971 - rmse: 0.4440\n",
            "Epoch 11/25000\n",
            "150/150 [==============================] - ETA: 0s - loss: 0.1999 - rmse: 0.4471\n",
            "Epoch 00011: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 24ms/step - loss: 0.1999 - rmse: 0.4471\n",
            "Epoch 12/25000\n",
            "149/150 [============================>.] - ETA: 0s - loss: 0.1921 - rmse: 0.4383\n",
            "Epoch 00012: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 0.1915 - rmse: 0.4376\n",
            "Epoch 13/25000\n",
            "150/150 [==============================] - ETA: 0s - loss: 0.1903 - rmse: 0.4362\n",
            "Epoch 00013: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 23ms/step - loss: 0.1903 - rmse: 0.4362\n",
            "Epoch 14/25000\n",
            "148/150 [============================>.] - ETA: 0s - loss: 0.1700 - rmse: 0.4123\n",
            "Epoch 00014: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 0.1689 - rmse: 0.4110\n",
            "Epoch 15/25000\n",
            "149/150 [============================>.] - ETA: 0s - loss: 0.1669 - rmse: 0.4086\n",
            "Epoch 00015: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 29ms/step - loss: 0.1671 - rmse: 0.4088\n",
            "Epoch 16/25000\n",
            "149/150 [============================>.] - ETA: 0s - loss: 0.1635 - rmse: 0.4044\n",
            "Epoch 00016: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 28ms/step - loss: 0.1630 - rmse: 0.4038\n",
            "Epoch 17/25000\n",
            "150/150 [==============================] - ETA: 0s - loss: 0.1593 - rmse: 0.3991\n",
            "Epoch 00017: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 24ms/step - loss: 0.1593 - rmse: 0.3991\n",
            "Epoch 18/25000\n",
            "150/150 [==============================] - ETA: 0s - loss: 0.1582 - rmse: 0.3977\n",
            "Epoch 00018: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.1582 - rmse: 0.3977\n",
            "Epoch 19/25000\n",
            "149/150 [============================>.] - ETA: 0s - loss: 0.1539 - rmse: 0.3922\n",
            "Epoch 00019: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 24ms/step - loss: 0.1537 - rmse: 0.3921\n",
            "Epoch 20/25000\n",
            "149/150 [============================>.] - ETA: 0s - loss: 0.1568 - rmse: 0.3960\n",
            "Epoch 00020: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 25ms/step - loss: 0.1570 - rmse: 0.3963\n",
            "Epoch 21/25000\n",
            "150/150 [==============================] - ETA: 0s - loss: 0.1410 - rmse: 0.3755\n",
            "Epoch 00021: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 26ms/step - loss: 0.1410 - rmse: 0.3755\n",
            "Epoch 22/25000\n",
            "150/150 [==============================] - ETA: 0s - loss: 0.1477 - rmse: 0.3844\n",
            "Epoch 00022: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 0.1477 - rmse: 0.3844\n",
            "Epoch 23/25000\n",
            "150/150 [==============================] - ETA: 0s - loss: 0.1425 - rmse: 0.3775\n",
            "Epoch 00023: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 0.1425 - rmse: 0.3775\n",
            "Epoch 24/25000\n",
            "149/150 [============================>.] - ETA: 0s - loss: 0.1593 - rmse: 0.3991\n",
            "Epoch 00024: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 0.1597 - rmse: 0.3996\n",
            "Epoch 25/25000\n",
            "148/150 [============================>.] - ETA: 0s - loss: 0.1685 - rmse: 0.4105\n",
            "Epoch 00025: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 23ms/step - loss: 0.1682 - rmse: 0.4101\n",
            "Epoch 26/25000\n",
            "149/150 [============================>.] - ETA: 0s - loss: 0.1640 - rmse: 0.4050\n",
            "Epoch 00026: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 28ms/step - loss: 0.1646 - rmse: 0.4057\n",
            "Epoch 27/25000\n",
            "150/150 [==============================] - ETA: 0s - loss: 0.1590 - rmse: 0.3987\n",
            "Epoch 00027: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 24ms/step - loss: 0.1590 - rmse: 0.3987\n",
            "Epoch 28/25000\n",
            "150/150 [==============================] - ETA: 0s - loss: 0.1461 - rmse: 0.3822\n",
            "Epoch 00028: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 28ms/step - loss: 0.1461 - rmse: 0.3822\n",
            "Epoch 29/25000\n",
            "150/150 [==============================] - ETA: 0s - loss: 0.1357 - rmse: 0.3684\n",
            "Epoch 00029: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 24ms/step - loss: 0.1357 - rmse: 0.3684\n",
            "Epoch 30/25000\n",
            "149/150 [============================>.] - ETA: 0s - loss: 0.1772 - rmse: 0.4209\n",
            "Epoch 00030: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 0.1768 - rmse: 0.4205\n",
            "Epoch 31/25000\n",
            "150/150 [==============================] - ETA: 0s - loss: 0.1780 - rmse: 0.4219\n",
            "Epoch 00031: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 0.1780 - rmse: 0.4219\n",
            "Epoch 32/25000\n",
            "148/150 [============================>.] - ETA: 0s - loss: 0.1861 - rmse: 0.4314\n",
            "Epoch 00032: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 24ms/step - loss: 0.1859 - rmse: 0.4312\n",
            "Epoch 33/25000\n",
            "150/150 [==============================] - ETA: 0s - loss: 0.1717 - rmse: 0.4143\n",
            "Epoch 00033: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 24ms/step - loss: 0.1717 - rmse: 0.4143\n",
            "Epoch 34/25000\n",
            "150/150 [==============================] - ETA: 0s - loss: 0.1430 - rmse: 0.3782\n",
            "Epoch 00034: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 0.1430 - rmse: 0.3782\n",
            "Epoch 35/25000\n",
            "148/150 [============================>.] - ETA: 0s - loss: 0.1719 - rmse: 0.4147\n",
            "Epoch 00035: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 28ms/step - loss: 0.1711 - rmse: 0.4136\n",
            "Epoch 36/25000\n",
            "149/150 [============================>.] - ETA: 0s - loss: 0.1584 - rmse: 0.3979\n",
            "Epoch 00036: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 23ms/step - loss: 0.1582 - rmse: 0.3977\n",
            "Epoch 37/25000\n",
            "149/150 [============================>.] - ETA: 0s - loss: 0.1526 - rmse: 0.3906\n",
            "Epoch 00037: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 5s 33ms/step - loss: 0.1523 - rmse: 0.3903\n",
            "Epoch 38/25000\n",
            "149/150 [============================>.] - ETA: 0s - loss: 0.1575 - rmse: 0.3968\n",
            "Epoch 00038: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 0.1570 - rmse: 0.3962\n",
            "Epoch 39/25000\n",
            "148/150 [============================>.] - ETA: 0s - loss: 0.1405 - rmse: 0.3749\n",
            "Epoch 00039: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 0.1412 - rmse: 0.3757\n",
            "Epoch 40/25000\n",
            "149/150 [============================>.] - ETA: 0s - loss: 0.1517 - rmse: 0.3895\n",
            "Epoch 00040: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 0.1518 - rmse: 0.3896\n",
            "Epoch 41/25000\n",
            "148/150 [============================>.] - ETA: 0s - loss: 0.1377 - rmse: 0.3710\n",
            "Epoch 00041: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 24ms/step - loss: 0.1370 - rmse: 0.3702\n",
            "Epoch 42/25000\n",
            "149/150 [============================>.] - ETA: 0s - loss: 0.1294 - rmse: 0.3598\n",
            "Epoch 00042: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 28ms/step - loss: 0.1293 - rmse: 0.3596\n",
            "Epoch 43/25000\n",
            "149/150 [============================>.] - ETA: 0s - loss: 0.1325 - rmse: 0.3640\n",
            "Epoch 00043: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 28ms/step - loss: 0.1324 - rmse: 0.3639\n",
            "Epoch 44/25000\n",
            "150/150 [==============================] - ETA: 0s - loss: 0.1277 - rmse: 0.3573\n",
            "Epoch 00044: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 28ms/step - loss: 0.1277 - rmse: 0.3573\n",
            "Epoch 45/25000\n",
            "149/150 [============================>.] - ETA: 0s - loss: 0.1223 - rmse: 0.3497\n",
            "Epoch 00045: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 24ms/step - loss: 0.1217 - rmse: 0.3489\n",
            "Epoch 46/25000\n",
            "150/150 [==============================] - ETA: 0s - loss: 0.1403 - rmse: 0.3745\n",
            "Epoch 00046: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 28ms/step - loss: 0.1403 - rmse: 0.3745\n",
            "Epoch 47/25000\n",
            "148/150 [============================>.] - ETA: 0s - loss: 0.1456 - rmse: 0.3816\n",
            "Epoch 00047: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 28ms/step - loss: 0.1454 - rmse: 0.3814\n",
            "Epoch 48/25000\n",
            "150/150 [==============================] - ETA: 0s - loss: 0.1194 - rmse: 0.3456\n",
            "Epoch 00048: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 0.1194 - rmse: 0.3456\n",
            "Epoch 49/25000\n",
            "148/150 [============================>.] - ETA: 0s - loss: 0.1236 - rmse: 0.3515\n",
            "Epoch 00049: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 24ms/step - loss: 0.1237 - rmse: 0.3518\n",
            "Epoch 50/25000\n",
            "150/150 [==============================] - ETA: 0s - loss: 0.1275 - rmse: 0.3571\n",
            "Epoch 00050: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 28ms/step - loss: 0.1275 - rmse: 0.3571\n",
            "Epoch 51/25000\n",
            "148/150 [============================>.] - ETA: 0s - loss: 0.1237 - rmse: 0.3516\n",
            "Epoch 00051: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 28ms/step - loss: 0.1252 - rmse: 0.3539\n",
            "Epoch 52/25000\n",
            "149/150 [============================>.] - ETA: 0s - loss: 0.1190 - rmse: 0.3450\n",
            "Epoch 00052: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 28ms/step - loss: 0.1188 - rmse: 0.3446\n",
            "Epoch 53/25000\n",
            "149/150 [============================>.] - ETA: 0s - loss: 0.1299 - rmse: 0.3605\n",
            "Epoch 00053: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 24ms/step - loss: 0.1295 - rmse: 0.3599\n",
            "Epoch 54/25000\n",
            "148/150 [============================>.] - ETA: 0s - loss: 0.1331 - rmse: 0.3649\n",
            "Epoch 00054: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 28ms/step - loss: 0.1327 - rmse: 0.3643\n",
            "Epoch 55/25000\n",
            "149/150 [============================>.] - ETA: 0s - loss: 0.1510 - rmse: 0.3886\n",
            "Epoch 00055: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 28ms/step - loss: 0.1509 - rmse: 0.3884\n",
            "Epoch 56/25000\n",
            "149/150 [============================>.] - ETA: 0s - loss: 0.1420 - rmse: 0.3768\n",
            "Epoch 00056: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 24ms/step - loss: 0.1429 - rmse: 0.3781\n",
            "Epoch 57/25000\n",
            "150/150 [==============================] - ETA: 0s - loss: 0.1595 - rmse: 0.3993\n",
            "Epoch 00057: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 24ms/step - loss: 0.1595 - rmse: 0.3993\n",
            "Epoch 58/25000\n",
            "149/150 [============================>.] - ETA: 0s - loss: 0.1367 - rmse: 0.3697\n",
            "Epoch 00058: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 28ms/step - loss: 0.1366 - rmse: 0.3696\n",
            "Epoch 59/25000\n",
            "148/150 [============================>.] - ETA: 0s - loss: 0.1583 - rmse: 0.3979\n",
            "Epoch 00059: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 28ms/step - loss: 0.1583 - rmse: 0.3979\n",
            "Epoch 60/25000\n",
            "149/150 [============================>.] - ETA: 0s - loss: 0.1819 - rmse: 0.4266\n",
            "Epoch 00060: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 0.1822 - rmse: 0.4269\n",
            "Epoch 61/25000\n",
            "150/150 [==============================] - ETA: 0s - loss: 0.1456 - rmse: 0.3816\n",
            "Epoch 00061: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 24ms/step - loss: 0.1456 - rmse: 0.3816\n",
            "Epoch 62/25000\n",
            "148/150 [============================>.] - ETA: 0s - loss: 0.1651 - rmse: 0.4064\n",
            "Epoch 00062: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 28ms/step - loss: 0.1651 - rmse: 0.4063\n",
            "Epoch 63/25000\n",
            "149/150 [============================>.] - ETA: 0s - loss: 0.1458 - rmse: 0.3818\n",
            "Epoch 00063: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 28ms/step - loss: 0.1453 - rmse: 0.3811\n",
            "Epoch 64/25000\n",
            "150/150 [==============================] - ETA: 0s - loss: 0.1401 - rmse: 0.3743\n",
            "Epoch 00064: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 0.1401 - rmse: 0.3743\n",
            "Epoch 65/25000\n",
            "148/150 [============================>.] - ETA: 0s - loss: 0.1424 - rmse: 0.3773\n",
            "Epoch 00065: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 24ms/step - loss: 0.1421 - rmse: 0.3770\n",
            "Epoch 66/25000\n",
            "149/150 [============================>.] - ETA: 0s - loss: 0.1477 - rmse: 0.3843\n",
            "Epoch 00066: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 28ms/step - loss: 0.1480 - rmse: 0.3847\n",
            "Epoch 67/25000\n",
            "149/150 [============================>.] - ETA: 0s - loss: 0.1319 - rmse: 0.3632\n",
            "Epoch 00067: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 0.1321 - rmse: 0.3635\n",
            "Epoch 68/25000\n",
            "149/150 [============================>.] - ETA: 0s - loss: 0.1399 - rmse: 0.3740\n",
            "Epoch 00068: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 30ms/step - loss: 0.1399 - rmse: 0.3740\n",
            "Epoch 69/25000\n",
            "149/150 [============================>.] - ETA: 0s - loss: 0.1456 - rmse: 0.3816\n",
            "Epoch 00069: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 25ms/step - loss: 0.1457 - rmse: 0.3817\n",
            "Epoch 70/25000\n",
            "149/150 [============================>.] - ETA: 0s - loss: 0.1517 - rmse: 0.3894\n",
            "Epoch 00070: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 28ms/step - loss: 0.1512 - rmse: 0.3888\n",
            "Epoch 71/25000\n",
            "149/150 [============================>.] - ETA: 0s - loss: 0.1321 - rmse: 0.3634\n",
            "Epoch 00071: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 0.1330 - rmse: 0.3646\n",
            "Epoch 72/25000\n",
            "150/150 [==============================] - ETA: 0s - loss: 0.1393 - rmse: 0.3732\n",
            "Epoch 00072: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 24ms/step - loss: 0.1393 - rmse: 0.3732\n",
            "Epoch 73/25000\n",
            "148/150 [============================>.] - ETA: 0s - loss: 0.1471 - rmse: 0.3835\n",
            "Epoch 00073: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 24ms/step - loss: 0.1475 - rmse: 0.3841\n",
            "Epoch 74/25000\n",
            "148/150 [============================>.] - ETA: 0s - loss: 0.1246 - rmse: 0.3530\n",
            "Epoch 00074: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 0.1241 - rmse: 0.3523\n",
            "Epoch 75/25000\n",
            "149/150 [============================>.] - ETA: 0s - loss: 0.1136 - rmse: 0.3371\n",
            "Epoch 00075: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 27ms/step - loss: 0.1131 - rmse: 0.3363\n",
            "Epoch 76/25000\n",
            "149/150 [============================>.] - ETA: 0s - loss: 0.1208 - rmse: 0.3476\n",
            "Epoch 00076: saving model to /content/drive/My Drive/daitan/implementation/largescale/tempmodels/tempmodels4.h5\n",
            "150/150 [==============================] - 4s 28ms/step - loss: 0.1205 - rmse: 0.3471\n",
            "Epoch 77/25000\n",
            " 54/150 [=========>....................] - ETA: 2s - loss: 0.1954 - rmse: 0.4421"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwNYGR2O_O-g"
      },
      "source": [
        "Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1GQDqQM_DrR"
      },
      "source": [
        "def read_audio(filepath, sample_rate, normalize=True):\n",
        "    # print(f\"Reading: {filepath}\").\n",
        "    audio, sr = librosa.load(filepath, sr=sample_rate)\n",
        "    if normalize:\n",
        "      div_fac = 1 / np.max(np.abs(audio)) / 3.0\n",
        "      audio = audio * div_fac\n",
        "    return audio, sr\n",
        "        \n",
        "def add_noise_to_clean_audio(clean_audio, noise_signal):\n",
        "    if len(clean_audio) >= len(noise_signal):\n",
        "        # print(\"The noisy signal is smaller than the clean audio input. Duplicating the noise.\")\n",
        "        while len(clean_audio) >= len(noise_signal):\n",
        "            noise_signal = np.append(noise_signal, noise_signal)\n",
        "\n",
        "    ## Extract a noise segment from a random location in the noise file\n",
        "    ind = np.random.randint(0, noise_signal.size - clean_audio.size)\n",
        "\n",
        "    noiseSegment = noise_signal[ind: ind + clean_audio.size]\n",
        "\n",
        "    speech_power = np.sum(clean_audio ** 2)\n",
        "    noise_power = np.sum(noiseSegment ** 2)\n",
        "    noisyAudio = clean_audio + np.sqrt(speech_power / noise_power) * noiseSegment\n",
        "    return noisyAudio\n",
        "\n",
        "def play(audio, sample_rate):\n",
        "    ipd.display(ipd.Audio(data=audio, rate=sample_rate))  # load a local WAV file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rg5Lbh9m_Svg"
      },
      "source": [
        "class FeatureExtractor:\n",
        "    def __init__(self, audio, *, windowLength, overlap, sample_rate):\n",
        "        self.audio = audio\n",
        "        self.ffT_length = windowLength\n",
        "        self.window_length = windowLength\n",
        "        self.overlap = overlap\n",
        "        self.sample_rate = sample_rate\n",
        "        self.window = scipy.signal.hamming(self.window_length, sym=False)\n",
        "\n",
        "    def get_stft_spectrogram(self):\n",
        "        return librosa.stft(self.audio, n_fft=self.ffT_length, win_length=self.window_length, hop_length=self.overlap,\n",
        "                            window=self.window, center=True)\n",
        "\n",
        "    def get_audio_from_stft_spectrogram(self, stft_features):\n",
        "        return librosa.istft(stft_features, win_length=self.window_length, hop_length=self.overlap,\n",
        "                             window=self.window, center=True)\n",
        "\n",
        "    def get_mel_spectrogram(self):\n",
        "        return librosa.feature.melspectrogram(self.audio, sr=self.sample_rate, power=2.0, pad_mode='reflect',\n",
        "                                           n_fft=self.ffT_length, hop_length=self.overlap, center=True)\n",
        "\n",
        "    def get_audio_from_mel_spectrogram(self, M):\n",
        "        return librosa.feature.inverse.mel_to_audio(M, sr=self.sample_rate, n_fft=self.ffT_length, hop_length=self.overlap,\n",
        "                                             win_length=self.window_length, window=self.window,\n",
        "                                             center=True, pad_mode='reflect', power=2.0, n_iter=32, length=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhzkizIU_Umy"
      },
      "source": [
        "cleanAudio, sr = read_audio('/content/drive/My Drive/daitan/implementation/smallscale/audio.wav', sample_rate=fs)\n",
        "print(\"Min:\", np.min(cleanAudio),\"Max:\",np.max(cleanAudio))\n",
        "ipd.Audio(data=cleanAudio, rate=sr) # load a local WAV file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWsdXVIK_bfI"
      },
      "source": [
        "noiseAudio, sr = read_audio('/content/drive/My Drive/daitan/implementation/smallscale/noise/00 - gold fronts-1.wav', sample_rate=fs)\n",
        "print(\"Min:\", np.min(noiseAudio),\"Max:\",np.max(noiseAudio))\n",
        "ipd.Audio(data=noiseAudio, rate=sr) # load a local WAV file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_mWvb2l_jdJ"
      },
      "source": [
        "cleanAudioFeatureExtractor = FeatureExtractor(cleanAudio, windowLength=windowLength, overlap=overlap, sample_rate=sr)\n",
        "stft_features = cleanAudioFeatureExtractor.get_stft_spectrogram()\n",
        "stft_features = np.abs(stft_features)\n",
        "print(\"Min:\", np.min(stft_features),\"Max:\",np.max(stft_features))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAcKPfug_lVE"
      },
      "source": [
        "noisyAudio = add_noise_to_clean_audio(cleanAudio, noiseAudio)\n",
        "ipd.Audio(data=noisyAudio, rate=fs) # load a local WAV file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3S8p3uez_ni4"
      },
      "source": [
        "def prepare_input_features(stft_features):\n",
        "    # Phase Aware Scaling: To avoid extreme differences (more than\n",
        "    # 45 degree) between the noisy and clean phase, the clean spectral magnitude was encoded as similar to [21]:\n",
        "    noisySTFT = np.concatenate([stft_features[:,0:numSegments-1], stft_features], axis=1)\n",
        "    stftSegments = np.zeros((numFeatures, numSegments , noisySTFT.shape[1] - numSegments + 1))\n",
        "\n",
        "    for index in range(noisySTFT.shape[1] - numSegments + 1):\n",
        "        stftSegments[:,:,index] = noisySTFT[:,index:index + numSegments]\n",
        "    return stftSegments"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdIBYgcg_ri0"
      },
      "source": [
        "noiseAudioFeatureExtractor = FeatureExtractor(noisyAudio, windowLength=windowLength, overlap=overlap, sample_rate=sr)\n",
        "noise_stft_features = noiseAudioFeatureExtractor.get_stft_spectrogram()\n",
        "\n",
        "# Paper: Besides, spectral phase was not used in the training phase.\n",
        "# At reconstruction, noisy spectral phase was used instead to\n",
        "# perform in- verse STFT and recover human speech.\n",
        "noisyPhase = np.angle(noise_stft_features)\n",
        "print(noisyPhase.shape)\n",
        "noise_stft_features = np.abs(noise_stft_features)\n",
        "\n",
        "mean = np.mean(noise_stft_features)\n",
        "std = np.std(noise_stft_features)\n",
        "noise_stft_features = (noise_stft_features - mean) / std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpMWHVT-_tW9"
      },
      "source": [
        "predictors = prepare_input_features(noise_stft_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioqVFzdG_wyy"
      },
      "source": [
        "predictors = np.reshape(predictors, (predictors.shape[0], predictors.shape[1], 1, predictors.shape[2]))\n",
        "predictors = np.transpose(predictors, (3, 0, 1, 2)).astype(np.float32)\n",
        "print('predictors.shape:', predictors.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evgJdjLe_yQS"
      },
      "source": [
        "STFTFullyConvolutional = saved_model.predict(predictors)\n",
        "print(STFTFullyConvolutional.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YbuBwOW_zbs"
      },
      "source": [
        "def revert_features_to_audio(features, phase, cleanMean=None, cleanStd=None):\n",
        "    # scale the outpus back to the original range\n",
        "    if cleanMean and cleanStd:\n",
        "        features = cleanStd * features + cleanMean\n",
        "\n",
        "    phase = np.transpose(phase, (1, 0))\n",
        "    features = np.squeeze(features)\n",
        "\n",
        "    # features = librosa.db_to_power(features)\n",
        "    features = features * np.exp(1j * phase)  # that fixes the abs() ope previously done\n",
        "\n",
        "    features = np.transpose(features, (1, 0))\n",
        "    return noiseAudioFeatureExtractor.get_audio_from_stft_spectrogram(features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5ca-Sat_2O_"
      },
      "source": [
        "denoisedAudioFullyConvolutional = revert_features_to_audio(STFTFullyConvolutional, noisyPhase, mean, std)\n",
        "print(\"Min:\", np.min(denoisedAudioFullyConvolutional),\"Max:\",np.max(denoisedAudioFullyConvolutional))\n",
        "ipd.Audio(data=denoisedAudioFullyConvolutional, rate=fs) # load a local WAV file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rjjp8-7T_4kB"
      },
      "source": [
        "f, (ax1, ax2, ax3) = plt.subplots(3, 1, sharey=True)\n",
        "\n",
        "ax1.plot(cleanAudio)\n",
        "ax1.set_title(\"Clean Audio\")\n",
        "\n",
        "ax2.plot(noisyAudio)\n",
        "ax2.set_title(\"Noisy Audio\")\n",
        "\n",
        "ax3.plot(denoisedAudioFullyConvolutional)\n",
        "ax3.set_title(\"Denoised Audio\")\n",
        "print(np.sqrt(np.mean((cleanAudio[:-42]-denoisedAudioFullyConvolutional)**2)))\n",
        "print(np.sqrt(np.mean((noisyAudio[:-42]-denoisedAudioFullyConvolutional)**2)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}